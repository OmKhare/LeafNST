{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model = YOLO('./models/custom-trained-yolov8-seg.pt')\n",
    "vgg = model.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameters in vgg:\n",
    "    vgg.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "model_layers = {}\n",
    "for name, layer in model._modules.items():\n",
    "    for name_l, layer_l in layer._modules.items():\n",
    "        for name_ll, layer_ll in layer_l._modules.items():\n",
    "            model_layers[str(i)] = layer_ll\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img_path, max_size = 640):\n",
    "\n",
    "  image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "  # if max(image.size) > max_size:\n",
    "  #   size = max_size\n",
    "\n",
    "  # else:\n",
    "  #   size = max(image.size)\n",
    "\n",
    "  img_transforms = T.Compose([\n",
    "      # T.Resize(size),\n",
    "      T.ToTensor(),  # (224, 224, 3) -> (3, 224, 224)\n",
    "      T.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                  std = [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "\n",
    "  image = img_transforms(image)\n",
    "\n",
    "  image = image.unsqueeze(0) # (3, 224, 224) -> (1, 3, 224, 224)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_p = preprocess('../images/af00bd10-d7ef-4076-90c8-28f2d6ff6aa2___RS_HL 8188.JPG')\n",
    "style_p = preprocess('../images/4dadb9f1-27b1-4d3c-8111-d1602febd585___JR_FrgE.S 8632.JPG')\n",
    "\n",
    "content_p = content_p.to(device)\n",
    "style_p = style_p.to(device)\n",
    "\n",
    "print(\"Content shape\", content_p.shape)\n",
    "print(\"Style shape\", style_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def deprocess(tensor):\n",
    "\n",
    "  image = tensor.to('cpu').clone()\n",
    "  image = image.numpy()\n",
    "  image = image.squeeze(0)\n",
    "  image = image.transpose(1, 2, 0)\n",
    "\n",
    "  # denormalizing the image\n",
    "  image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "\n",
    "  image = image.clip(0, 1)\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_d = deprocess(content_p)\n",
    "style_d = deprocess(style_p)\n",
    "\n",
    "print(\"Deprocess content:\", content_d.shape)\n",
    "print(\"Deprocess style:\", style_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "ax1.imshow(content_d)\n",
    "ax2.imshow(style_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model):\n",
    "\n",
    "  layers = {\n",
    "\n",
    "            '0' : 'conv1_1',\n",
    "            '1' : 'conv2_1',\n",
    "            '2' : 'conv3_1',\n",
    "            '4' : 'conv4_1',\n",
    "            '7' : 'conv4_2',\n",
    "            '8' : 'conv5_1'\n",
    "  }\n",
    "\n",
    "  x = image\n",
    "\n",
    "  Features = {}\n",
    "  i = 0\n",
    "\n",
    "  for name in model_layers.keys():\n",
    "\n",
    "    x = model_layers[name](x)\n",
    "\n",
    "    if name in layers:\n",
    "      Features[layers[name]] = x\n",
    "    i += 1\n",
    "  \n",
    "    if (i > 8):\n",
    "      break\n",
    "    \n",
    "\n",
    "  return Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_f = get_features(content_p, vgg)\n",
    "style_f = get_features(style_p, vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "\n",
    "  b, c, h, w = tensor.size()\n",
    "  tensor = tensor.view(c, h*w)\n",
    "\n",
    "  gram = torch.mm(tensor, tensor.t())\n",
    "\n",
    "  return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_grams = { layer : gram_matrix(style_f[layer]) for layer in style_f }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(target_conv4_2, content_conv4_2):\n",
    "\n",
    "  loss = torch.mean((target_conv4_2 - content_conv4_2)**2)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weights = {\n",
    "\n",
    "    'conv1_1' : 0.2,\n",
    "    'conv2_1' : 0.2,\n",
    "    'conv3_1' : 0.5,\n",
    "    'conv4_1' : 1.0,\n",
    "    'conv5_1' : 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style_weights, target_features, style_grams):\n",
    "  loss = 0\n",
    "\n",
    "  for layer in style_weights:\n",
    "    target_f = target_features[layer]\n",
    "    target_gram = gram_matrix(target_f)\n",
    "    style_gram = style_grams[layer]\n",
    "    b, c, h, w = target_f.shape\n",
    "    layer_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "    loss += layer_loss/(c*h*w)\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = content_p.clone().requires_grad_(True).to(device)\n",
    "target_f = get_features(target, vgg)\n",
    "print(\"Content Loss: \", content_loss(target_f['conv4_2'], content_f['conv4_2']))\n",
    "print(\"Style Loss: \", style_loss(style_weights, target_f, style_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam([target], lr = 0.008)\n",
    "\n",
    "alpha = 1\n",
    "beta = 1e5\n",
    "\n",
    "epochs = 5000\n",
    "show_every = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(c_loss, s_loss, alpha, beta):\n",
    "  loss = alpha * c_loss + beta * s_loss\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(epochs):\n",
    "  target_f = get_features(target, vgg)\n",
    "\n",
    "  c_loss = content_loss(target_f['conv4_2'], content_f['conv4_2'])\n",
    "  s_loss = style_loss(style_weights, target_f, style_grams)\n",
    "\n",
    "  t_loss = total_loss(c_loss, s_loss, alpha, beta)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  t_loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if i % show_every == 0:\n",
    "    print(\"Total loss at epoch {}: {}\".format(i, t_loss))\n",
    "    results.append(deprocess(target.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_copy = deprocess(target.detach())\n",
    "content_copy = deprocess(content_p)\n",
    "print(target_copy.shape)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "# ax1.imshow(target_copy)\n",
    "# ax2.imshow(content_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(target_copy.type())\n",
    "target_copy_1 = target_copy.astype(np.float)\n",
    "print(target_copy.shape)\n",
    "img = np.dstack((target_copy_1,target_copy_1,target_copy_1))\n",
    "print(img.shape)\n",
    "img = target_copy_1[:,:,:3]\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def imread(path):\n",
    "    img = cv2.imread(path).astype(np.float)\n",
    "    if len(img.shape) == 2:\n",
    "        # grayscale\n",
    "        img = np.dstack((img,img,img))\n",
    "    elif img.shape[2] == 4:\n",
    "        # PNG with alpha channel\n",
    "        img = img[:,:,:3]\n",
    "    return img\n",
    "\n",
    "def imsave(path, img):\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    Image.fromarray(img).save(path, quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "device: str = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "model = YOLO('./models/custom-trained-yolov8-seg.pt')  # load a pretrained YOLOv8n segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "imgcon = cv2.imread('content.png')\n",
    "imgcon = cv2.cvtColor(imgcon, cv2.COLOR_BGR2RGB)\n",
    "H, W, _ = imgcon.shape\n",
    "results = model(imgcon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for result in results:\n",
    "    for j, mask in enumerate(result.masks.data):\n",
    "        mask = mask.cpu().numpy() * 255\n",
    "        mask  =cv2.resize(mask, (W, H))\n",
    "        cv2.imwrite('./mask.png', mask)\n",
    "        if i == 0:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtar = cv2.imread(\"target.png\")\n",
    "imgtar = cv2.cvtColor(imgtar, cv2.COLOR_BGR2RGB)\n",
    "imgtar = imgtar.astype(np.uint8)\n",
    "imgtar = cv2.resize(imgtar, (W, H))\n",
    "mask = cv2.imread('./mask.png', cv2.IMREAD_GRAYSCALE)\n",
    "_, binary_mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "masked_overlay = cv2.bitwise_or(imgtar, imgtar, mask=binary_mask)\n",
    "masked_overlay = masked_overlay.astype(np.uint8)\n",
    "inverted_mask = cv2.bitwise_not(binary_mask.astype(np.uint8))\n",
    "roi = cv2.bitwise_and(imgcon, imgcon, mask=inverted_mask)\n",
    "result_image = cv2.add(roi, masked_overlay)\n",
    "imsave('./final_output.png', result_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
